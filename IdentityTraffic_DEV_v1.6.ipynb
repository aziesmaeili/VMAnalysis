{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import pyodbc\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import  linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from itertools import product\n",
    "import statsmodels.api as sm\n",
    "import psutil\n",
    "import gc\n",
    "from pyearth import Earth\n",
    "from io import StringIO, BytesIO\n",
    "verbose=True\n",
    "savelocally=False\n",
    "today = datetime.date.today()\n",
    "month=today.month\n",
    "year=today.year\n",
    "metrics=[]\n",
    "\n",
    "version=1.5\n",
    "# V1.2 : Adapted script for new server classification method\n",
    "# V1.2 : Changed parameter collection to be a list of lists rather than a dictionary\n",
    "# V1.2 : Removed removeoutlier and serverclassification functions as they're no longer needed\n",
    "# V1.5 : Adapted getinputfile_fromblob to read from blob but not save locally\n",
    "# V1.5 : Changed folder path for push to blob functions\n",
    "# V1.6 : By default all pushtoblob functions will not save locally unless savelocally is set to True\n",
    "\n",
    "# V1.2 : changed file patterns for new file and removed parameter file pattern\n",
    "\n",
    "def getblobfilename(tenant,role,asofdate):\n",
    "    blobdatafilepattern = \"{:#tenant#/efficiencyview/%Y/%m/Classifiedinput/#tenant#_Classified_Inputdata_for_rps_prediction_#role#_%Y%m%d.csv}\"\n",
    "    blobdatafile= blobdatafilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "    return blobdatafile\n",
    "\n",
    "def makedirectory(tenant,role,asofdate):\n",
    "    dirpath = \"{:./efficiencyview/#tenant#/#role#/%Y/%m/}\"\n",
    "    path= dirpath.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def getinputfile_fromblob(storageaccountname,accountkey,container,tenant,role,asofdate):\n",
    "    byte_stream = BytesIO()\n",
    "    blobservice = BlockBlobService(storageaccountname,accountkey)\n",
    "    blobdatafile=getblobfilename(tenant,role,asofdate)\n",
    "    blobservice.get_blob_to_stream(container_name=container, blob_name=blobdatafile, stream=byte_stream)\n",
    "    byte_stream.seek(0)\n",
    "    dfdata=pd.read_csv(byte_stream)\n",
    "    byte_stream.close()\n",
    "    return dfdata\n",
    "\n",
    "def correlation(Data):\n",
    "    datacorr=Data[['Requests_sec','PercentProcessorTime','RequestExecutionTime','Percent_Memory_Utilized']]\n",
    "    corr = datacorr.corr()\n",
    "    datacorr.corr()\n",
    "    return datacorr.corr()\n",
    "\n",
    "# V1.2 : The new data already has a date column - this function creates date variables that are used in data formatting\n",
    "def dates(df):\n",
    "    data_dates= pd.to_datetime(df['Interval'], format = \"%Y-%m-%d\")\n",
    "    metrics.append(['number of unique dates',data_dates.dt.normalize().nunique()])\n",
    "    mindate= data_dates.min().strftime('%Y-%#m-%d')\n",
    "    maxdate= data_dates.max().strftime('%Y-%#m-%d')\n",
    "    month =data_dates.min().strftime('%b')\n",
    "    year = data_dates.min().strftime('%Y')\n",
    "    daterange=month + \" \" + year +\"(\"+ month +\")\"\n",
    "    return mindate, maxdate, month, year, daterange\n",
    "\n",
    "# V1.2 : created a function to collect raw data info to clean efficiency function\n",
    "def rawdatametrics(df):\n",
    "    metrics.append(['number of raw data rows', df.shape[0]])\n",
    "    metrics.append(['number of unique data centers', df.DataCenter.nunique()])\n",
    "    metrics.append(['number of unique active servers', df.ServerName.nunique()])\n",
    "    metrics.append(['server classification method', \"TotalTransactions\"])\n",
    "\n",
    "# V1.2 : created a function to create RAM column to clean efficiency view\n",
    "def addmemoryutilized(df):\n",
    "    maxmemory=df['AvailableMBytes'].max()\n",
    "    df['Percent_Memory_Utilized']=((maxmemory-df['AvailableMBytes'])/maxmemory)*100\n",
    "    return df\n",
    "\n",
    "def filterbySTD(Data):    \n",
    "    Data['RPS2']=np.where(Data['Requests_sec']==0,0.00001,Data['Requests_sec'])\n",
    "    Data['PercentProcessorTime']=np.where(Data['PercentProcessorTime']==0,0.00001,Data['PercentProcessorTime'])\n",
    "    Data['rt']=Data['RPS2']/Data['PercentProcessorTime']\n",
    "    Data=Data[(Data['Requests_sec'] !=0)&(Data['RequestExecutionTime']!=0)]\n",
    "    x, _ = stats.boxcox(Data['rt'])\n",
    "    Data=Data.drop(['RPS2','rt'], axis=1)\n",
    "    gc.collect()\n",
    "    Data['tp']=x\n",
    "    median=np.median(Data['tp'])\n",
    "    sd = np.std(Data['tp'])\n",
    "    Data['sd_rt']=(Data['tp']-median)/sd\n",
    "    Data['sd_range2']=round(Data['sd_rt'],1)\n",
    "    Data=Data.drop(['tp','sd_rt'], axis=1)\n",
    "    gc.collect()\n",
    "    if(verbose):\n",
    "        print(\"% of Data between standard deviation range -1 & 1 :\",len(Data[(Data['sd_range2']>-1)&(Data['sd_range2']<1)])/len(Data))\n",
    "        print(\"% of Data between standard deviation range -2 & 2 :\",len(Data[(Data['sd_range2']>-2)&(Data['sd_range2']<2)])/len(Data))\n",
    "        print(\"% of Data between standard deviation range -3 & 3 :\",len(Data[(Data['sd_range2']>-3)&(Data['sd_range2']<3)])/len(Data))\n",
    "    Data['Outlier']=np.where((Data['sd_range2']<=-3)|(Data['sd_range2']>=3),1,0)\n",
    "    cleanedData=Data[Data['Outlier']==0]\n",
    "    if(verbose):\n",
    "        print(\"Number of rows after cleaning data:\",cleanedData.shape[0])\n",
    "    cleanedData=cleanedData.drop(['Outlier','sd_range2'], axis=1)\n",
    "    metrics.append(['number of rows after filtering by stdev', cleanedData.shape[0]])\n",
    "    return cleanedData\n",
    "\n",
    "# V1.1 : Changed X_train to df for memory optimization. This is reducing the memory required by not creating an extra data frame X_train\n",
    "def filterbyInfluence(df, random_State, SamplePercent,path): \n",
    "    y=df[['Requests_sec']]\n",
    "    y=pd.DataFrame(y)\n",
    "    df=df[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime','TotalTransactions']]\n",
    "    test_size=(1-SamplePercent)\n",
    "    df, X_test, y_train, y_test = train_test_split(df, y, test_size=test_size, random_state=random_State)\n",
    "    clf= sm.OLS(y_train, sm.add_constant(df[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']])).fit()\n",
    "    df['model_leverage'] = clf.get_influence().hat_matrix_diag\n",
    "    df['model_cooks'] = clf.get_influence().cooks_distance[0]\n",
    "    df=pd.concat([df, y_train], axis=1, join ='inner')\n",
    "    sampletestdata=pd.concat([X_test, y_test], axis=1, join ='inner')\n",
    "    \n",
    "    #inf_data = df[(df.model_leverage >= (2/df.shape[0])) & (df.model_cooks >= 3*(df.model_cooks.mean())) > (df.model_cooks <= 1)]\n",
    "    #inf_data.to_csv(os.path.join(path,'inf_data.csv'),index=False)\n",
    "    df = df[(df.model_leverage < (2/df.shape[0])) | (df.model_cooks < 3*(df.model_cooks.mean())) | (df.model_cooks <= 1)]\n",
    "\n",
    "    df=df.drop(['model_leverage','model_cooks'], axis=1)\n",
    "    metrics.append(['random_state',random_State])\n",
    "    metrics.append(['number of rows after filtering by influence', (df.shape[0] + sampletestdata.shape[0])])\n",
    "    metrics.append(['number of sample rows', df.shape[0]])\n",
    "    metrics.append(['sample dataset percentage', Sample_Percent])\n",
    "    if(verbose):\n",
    "        print(\"Number of rows after filtering by influence:\",(df.shape[0] + sampletestdata.shape[0]))\n",
    "    if (verbose):\n",
    "        print('sample count',df.shape[0])\n",
    "    return df,sampletestdata\n",
    "\n",
    "# V1.1 : Eliminated the creation of new data frame X_train and X_test for memory.\n",
    "# V1.1 : Removed the quantile stats as they are getting collected as a seperate function.\n",
    "# V1.1 : Restructured the code for memory optimization.\n",
    "# V1.1 : Rounded off the values of RET and Memory steps and end values for efficiency preditions.\n",
    "# V1.2 : Still using linear regression for now - will change this function when we finalize which model to use\n",
    "# V1.2 : Split the function into two - one to fit the model and another to create prediction data. sample_linear_regression now returned a fitted model\n",
    "def linear_regression(df, sampletestdf, random_State, path,prediction,cv): \n",
    "    modeltype='LR'\n",
    "    y_train = df[['Requests_sec']]\n",
    "    sampletraindf = df[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']]\n",
    "    gc.collect()\n",
    "    y_true= sampletestdf['Requests_sec'].as_matrix()\n",
    "    gc.collect()\n",
    "    fitted = linear_model.LinearRegression(fit_intercept=True, normalize=True, copy_X=False, n_jobs=-1)\n",
    "    fitted.fit(sampletraindf, y_train)\n",
    "    sampletestdf['Requests_sec_LR'] = fitted.predict(sampletestdf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']])\n",
    "    prediction['RPS']=fitted.predict(prediction[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']])\n",
    "    gc.collect()\n",
    "    prediction['RPS']=np.where(prediction['RPS'] < 0,0,prediction['RPS'])\n",
    "    fitted_cv = linear_model.LinearRegression(fit_intercept=True, normalize=True, copy_X=False, n_jobs=-1)\n",
    "    cvMSE=-np.mean(cross_val_score(fitted_cv,sampletraindf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']] , y_train , cv=cv, scoring='neg_mean_squared_error'))\n",
    "    metrics.append(['sample cv MSE', cvMSE])\n",
    "    r2=np.mean(cross_val_score(fitted_cv,sampletraindf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']] , y_train , cv=cv, scoring='r2'))\n",
    "    metrics.append(['sample cv R2',r2])\n",
    "    fitted_cv = None\n",
    "    gc.collect()\n",
    "    MAE_LR=mean_absolute_error(y_true, sampletestdf['Requests_sec_LR'])\n",
    "    metrics.append(['sample MAE LR',MAE_LR])\n",
    "    MSE_LR=np.mean((sampletestdf['Requests_sec_LR'] - y_true) ** 2)\n",
    "    metrics.append(['sample MSE LR',MSE_LR])\n",
    "    RMSE_LR=np.sqrt(mean_squared_error(y_true, sampletestdf['Requests_sec_LR']))\n",
    "    metrics.append(['sample RMSE LR',RMSE_LR])\n",
    "    R2_LR=r2_score(y_true, sampletestdf['Requests_sec_LR'])\n",
    "    metrics.append(['sample R2 LR',R2_LR])\n",
    "    MAPE_LR=np.mean(np.abs((y_true - sampletestdf['Requests_sec_LR']) / y_true))\n",
    "    metrics.append(['sample MAPE LR',MAPE_LR])\n",
    "    metrics.append(['fitting model', modeltype])\n",
    "    metrics.append(['number of sample train rows', sampletraindf.shape[0]])\n",
    "    metrics.append(['number of sample test rows', sampletestdf.shape[0]])\n",
    "    metrics.append(['sample test data size', round(sampletestdf.shape[0]/ (sampletraindf.shape[0] + sampletestdf.shape[0]), 2)])\n",
    "    params=fitted.get_params()\n",
    "    params_keys = list(params.keys())\n",
    "    params_items = list(params.values())\n",
    "    params_keys = [\"model parameter \"+ x for x in params_keys]\n",
    "    params=list(zip(params_keys, params_items))\n",
    "    [metrics.append(list(elem)) for elem in params]\n",
    "    fitted = None\n",
    "\n",
    "    if(verbose):\n",
    "        print('cross val mse: %f' %cvMSE)\n",
    "        print('R2: %.2f' %r2)\n",
    "        print('mean absoluter error: %f' %mae)\n",
    "        print('Mean squared error: %.2f' %mse)\n",
    "    mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "    plt.scatter(sampletestdf['PercentProcessorTime'], y_test,  color='black')\n",
    "    plt.plot(sampletestdf['PercentProcessorTime'], sampletestdf['Requests_sec_LR'], color='blue', linewidth=3)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plotname=\"Plot_cleanData_vs_LR.png\"\n",
    "    plt.savefig(os.path.join(path,plotname))\n",
    "    sampletestdf=None\n",
    "    gc.collect()\n",
    "    return prediction\n",
    "\n",
    "def mars(df, sampletestdf, random_State, path, prediction): \n",
    "    modeltype='MARS'\n",
    "    y_train = df[['Requests_sec']]\n",
    "    sampletraindf = df[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']]\n",
    "    gc.collect()\n",
    "    y_true= sampletestdf['Requests_sec'].as_matrix()\n",
    "    model_mars = Earth(max_terms=30)\n",
    "    print(\"Fitting MARS model\")\n",
    "    model_mars.fit(sampletraindf,y_train)\n",
    "    print(model_mars.summary())\n",
    "    sampletestdf['Requests_sec_Mars'] = model_mars.predict(sampletestdf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']])\n",
    "    prediction['RPS']=model_mars.predict(prediction)\n",
    "    model_mars= None\n",
    "    gc.collect()\n",
    "    prediction['RPS']=np.where(prediction['RPS'] < 0,0,prediction['RPS'])\n",
    "    MAE_Mars=mean_absolute_error(y_true, sampletestdf['Requests_sec_Mars'])\n",
    "    metrics.append(['sample MAE MARS',MAE_Mars])\n",
    "    MSE_Mars=np.mean((sampletestdf['Requests_sec_Mars'] - y_true) ** 2)\n",
    "    metrics.append(['sample MSE MARS',MSE_Mars])\n",
    "    RMSE_Mars = np.sqrt(mean_squared_error(y_true, sampletestdf['Requests_sec_Mars']))\n",
    "    metrics.append(['sample RMSE MARS',RMSE_Mars])\n",
    "    R2_Mars = r2_score(y_true, sampletestdf['Requests_sec_Mars'])\n",
    "    metrics.append(['sample R2 MARS',R2_Mars])\n",
    "    MAPE_Mars=np.mean(np.abs((y_true - sampletestdf['Requests_sec_Mars']) / y_true))\n",
    "    metrics.append(['sample MAPE MARS',MAPE_Mars])\n",
    "    metrics.append(['sample random state', random_State])\n",
    "    metrics.append(['fitting model',modeltype])\n",
    "    metrics.append(['number of sample train rows', sampletraindf.shape[0]])\n",
    "    metrics.append(['number of sample test rows', sampletestdf.shape[0]])\n",
    "    metrics.append(['sample test data size', round(sampletestdf.shape[0]/ (sampletraindf.shape[0] + sampletestdf.shape[0]), 2)])\n",
    "    gc.collect()\n",
    "    if(verbose):\n",
    "        print('R2: %.2f' %R2_Mars)\n",
    "        print('mean absoluter error: %f' %MAE_Mars)\n",
    "        print('Mean squared error: %.2f' %MSE_Mars)\n",
    "    gc.collect()\n",
    "    sampletestdf = sampletestdf[['PercentProcessorTime', 'Percent_Memory_Utilized','RequestExecutionTime', 'Requests_sec','Requests_sec_Mars']]\n",
    "    sampletestdf.columns = ['CPU','Memory','RET','ActualRPS','RPSPredictMars']\n",
    "    plt.scatter(sampletestdf['CPU'], sampletestdf['ActualRPS'],  color='black')\n",
    "    plt.plot(sampletestdf['CPU'], sampletestdf['RPSPredictMars'], color='blue', linewidth=3)\n",
    "    plt.xlabel(\"CPU\")\n",
    "    plt.ylabel(\"RPS\")\n",
    "    plotname=\"Plot_cleanData_vs_MARS.png\"\n",
    "    plt.savefig(os.path.join(path,plotname))\n",
    "    plt.clf()\n",
    "    sampletestdf=None\n",
    "    gc.collect()\n",
    "    return prediction\n",
    "\n",
    "def comparemodels(sampletraindf, sampletestdf, prediction, SampleStartDate, SampleEndDate, tenant, role, random_State, path, cv):    \n",
    "    y_train = sampletraindf[['Requests_sec']]\n",
    "    sampletraindf = sampletraindf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']]\n",
    "    gc.collect()\n",
    "    y_true= sampletestdf['Requests_sec'].as_matrix()\n",
    "    metrics.append(['number of sample train rows', sampletraindf.shape[0]])\n",
    "    metrics.append(['number of sample test rows',sampletestdf.shape[0]])\n",
    "    metrics.append(['sample test data size', round(sampletestdf.shape[0]/ (sampletraindf.shape[0] + sampletestdf.shape[0]), 2)])\n",
    "    metrics.append(['sample cv folds', cv])\n",
    "    metrics.append(['sample random state', random_State])\n",
    "    #MARSvsLR : Set verbose to >=1 to trace the model execution.\n",
    "    #model_mars = Earth(max_terms=30,verbose=2)\n",
    "    print(\"Fitting MARS model\")\n",
    "    model_mars = Earth(max_terms=30)\n",
    "    model_mars.fit(sampletraindf,y_train)\n",
    "    print(model_mars.summary())\n",
    "    sampletestdf['Requests_sec_Mars'] = model_mars.predict(sampletestdf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']])\n",
    "    sampletestdf['Requests_sec_Mars'] = np.where(sampletestdf['Requests_sec_Mars'] < 0,0,sampletestdf['Requests_sec_Mars'])\n",
    "    MAE_Mars=mean_absolute_error(y_true, sampletestdf['Requests_sec_Mars'])\n",
    "    metrics.append(['sample MAE MARS',MAE_Mars])\n",
    "    MSE_Mars=np.mean((sampletestdf['Requests_sec_Mars'] - y_true) ** 2)\n",
    "    metrics.append(['sample MSE MARS',MSE_Mars])\n",
    "    RMSE_Mars = np.sqrt(mean_squared_error(y_true, sampletestdf['Requests_sec_Mars']))\n",
    "    metrics.append(['sample RMSE MARS',RMSE_Mars])\n",
    "    R2_Mars = r2_score(y_true, sampletestdf['Requests_sec_Mars'])\n",
    "    metrics.append(['sample R2 MARS',R2_Mars])\n",
    "    MAPE_Mars=np.mean(np.abs((y_true - sampletestdf['Requests_sec_Mars']) / y_true))\n",
    "    metrics.append(['sample MAPE MARS',MAPE_Mars])\n",
    "    prediction['RPS_MARS']=model_mars.predict(prediction)\n",
    "    prediction['RPS_MARS']=np.where(prediction['RPS_MARS'] < 0,0,prediction['RPS_MARS'])\n",
    "    model_mars = None\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Fitting LR model\")\n",
    "    fitted = linear_model.LinearRegression(fit_intercept=True, normalize=True, copy_X=False, n_jobs=-1)\n",
    "    fitted.fit(sampletraindf, y_train)\n",
    "    params=fitted.get_params()\n",
    "    params_keys = list(params.keys())\n",
    "    params_items = list(params.values())\n",
    "    params_keys = [\"model parameter \"+ x for x in params_keys]\n",
    "    params=list(zip(params_keys, params_items))\n",
    "    [metrics.append(list(elem)) for elem in params]\n",
    "    sampletestdf['Requests_sec_LR'] = fitted.predict(sampletestdf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']])\n",
    "    sampletestdf['Requests_sec_LR'] = np.where(sampletestdf['Requests_sec_LR'] < 0,0,sampletestdf['Requests_sec_LR'])\n",
    "    fitted_cv = linear_model.LinearRegression(fit_intercept=True, normalize=True, copy_X=False, n_jobs=-1)\n",
    "    cvMSE=-np.mean(cross_val_score(fitted_cv,sampletraindf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']] , y_train , cv=cv, scoring='neg_mean_squared_error'))\n",
    "    metrics.append(['sample cv MSE LR',cvMSE])\n",
    "    r2=np.mean(cross_val_score(fitted_cv,sampletraindf[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']] , y_train , cv=cv, scoring='r2'))\n",
    "    metrics.append(['sample cv R2 LR',r2])\n",
    "    MAE_LR=mean_absolute_error(y_true, sampletestdf['Requests_sec_LR'])\n",
    "    metrics.append(['sample MAE LR',MAE_LR])\n",
    "    MSE_LR=np.mean((sampletestdf['Requests_sec_LR'] - y_true) ** 2)\n",
    "    metrics.append(['sample MSE LR',MSE_LR])\n",
    "    RMSE_LR=np.sqrt(mean_squared_error(y_true, sampletestdf['Requests_sec_LR']))\n",
    "    metrics.append(['sample RMSE LR',RMSE_LR])\n",
    "    R2_LR=r2_score(y_true, sampletestdf['Requests_sec_LR'])\n",
    "    metrics.append(['sample R2 LR',R2_LR])\n",
    "    MAPE_LR=np.mean(np.abs((y_true - sampletestdf['Requests_sec_LR']) / y_true))\n",
    "    metrics.append(['sample MAPE LR',MAPE_LR])\n",
    "    prediction['RPS_LR']=fitted.predict(prediction[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']])\n",
    "    prediction['RPS_LR']=np.where(prediction['RPS_LR'] < 0,0,prediction['RPS_LR'])\n",
    "    fitted = None\n",
    "    sampletraindf = None\n",
    "    gc.collect()\n",
    " \n",
    "    sampletestdf = sampletestdf[['PercentProcessorTime', 'Percent_Memory_Utilized','RequestExecutionTime', 'Requests_sec','Requests_sec_LR', 'Requests_sec_Mars']]\n",
    "    sampletestdf.columns = ['CPU','Memory','RET','ActualRPS','RPSPredictLR','RPSPredictMars']\n",
    "    #generate_test_data_summary(df=sampletestdf,SampleStartDate = SampleStartDate,SampleEndDate = SampleEndDate,cpubucket=2,memorybucket=2,retbucket=10,path=path,tenant=tenant,role=role)\n",
    "    #enddate = SampleEndDate[:10].replace(\"-\",\"\")\n",
    "    #sampletestdf['SampleStartDate'] = SampleStartDate\n",
    "    #sampletestdf['SampleEndDate'] = SampleEndDate\n",
    "    #sampletestdf['Tenant'] = tenant\n",
    "    #sampletestdf['Role'] = role\n",
    "    #sampletestfile = 'SampleTest'+role+enddate+'.csv'\n",
    "    #sampletestdf.to_csv(os.path.join(path,sampletestfile),index=False)\n",
    "    #error_metrics = pd.DataFrame([[\"Mars\",MAE_Mars,MSE_Mars,RMSE_Mars,R2_Mars,MAPE_Mars],[\"LR\",MAE_LR,MSE_LR,RMSE_LR,R2_LR,MAPE_LR]],columns=['Model','MAE','MSE','RMSE','R2','MAPE'])\n",
    "    #errormetricsfile = 'ModelComparisionErrorMetrics'+role+enddate+'.csv'\n",
    "    #error_metrics.to_csv(os.path.join(path,errormetricsfile),index=False)\n",
    "    #sampletestdf = None\n",
    "    gc.collect()\n",
    "    if(verbose):\n",
    "        print('cross val mse: %f' %cvMSE)\n",
    "        print('R2: %.2f' %R2_LR)\n",
    "        print('mean absoluter error: %f' %MAE_LR)\n",
    "        print('Mean squared error: %.2f' %MSE_LR)\n",
    "        \n",
    "    #print(\"Plotting LR model\")\n",
    "    #mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "    #plt.scatter(sampletestdf['CPU'], sampletestdf['ActualRPS'],  color='black')\n",
    "    #plt.plot(sampletestdf['CPU'], sampletestdf['RPSPredictLR'], color='blue', linewidth=3)\n",
    "    #plt.xlabel(\"CPU\")\n",
    "    #plt.ylabel(\"RPS\")\n",
    "    #plotname=\"Plot_cleanData_vs_LR.png\"\n",
    "    #plt.savefig(os.path.join(path,plotname))\n",
    "    #plt.clf()\n",
    "    #plt.scatter(sampletestdf['CPU'], sampletestdf['ActualRPS'],  color='black')\n",
    "    #plt.plot(sampletestdf['CPU'], sampletestdf['RPSPredictMars'], color='blue', linewidth=3)\n",
    "    #plt.xlabel(\"CPU\")\n",
    "    #plt.ylabel(\"RPS\")\n",
    "    #plotname=\"Plot_cleanData_vs_MARS.png\"\n",
    "    #plt.savefig(os.path.join(path,plotname))\n",
    "    #plt.clf()\n",
    "    sampletestdf = None\n",
    "    return prediction\n",
    "\n",
    "# V1.2 : new function to create pseudo data\n",
    "def generateprediction(df):\n",
    "    RET_lower=round(np.percentile(df['RequestExecutionTime'],10),0)\n",
    "    RET_upper=round(np.percentile(df['RequestExecutionTime'],90),0)\n",
    "    RAM_lower=round(np.percentile(df['Percent_Memory_Utilized'],10),0)\n",
    "    RAM_upper=round(np.percentile(df['Percent_Memory_Utilized'],90),0)\n",
    "    RET_step = round((RET_upper-RET_lower)/10,0)\n",
    "    RAM_step = round((RAM_upper-RAM_lower)/10,0)\n",
    "    RET_step = np.where(RET_step == 0,1,RET_step)\n",
    "    RAM_step = np.where(RAM_step == 0,1,RAM_step)\n",
    "    #1.2 added step size to printed values\n",
    "    if(verbose):\n",
    "        print('RET_lower: %d' %RET_lower, 'RET_upper: %d' %RET_upper, 'RET_step: %d' %RET_step)\n",
    "        print('RAM_lower: %d' %RAM_lower, 'RAM_upper: %d' %RAM_upper, 'RAM_step: %d' %RAM_step)\n",
    "    PercentProcessorTime = np.arange(5, 85, 5)\n",
    "    RequestExecutionTime = np.arange(RET_lower,(RET_upper+RET_step), RET_step)\n",
    "    Percent_Memory_Utilized = np.arange(RAM_lower,(RAM_upper+RAM_step), RAM_step)\n",
    "    PercentProcessorTime = PercentProcessorTime.astype('int')\n",
    "    RequestExecutionTime = RequestExecutionTime.astype('int')\n",
    "    Percent_Memory_Utilized = Percent_Memory_Utilized.astype('int')\n",
    "    prediction={'PercentProcessorTime':PercentProcessorTime,'RequestExecutionTime':RequestExecutionTime,'Percent_Memory_Utilized':Percent_Memory_Utilized}\n",
    "    prediction=pd.DataFrame([row for row in product(*prediction.values())],columns=prediction.keys())\n",
    "    prediction=prediction[['PercentProcessorTime','Percent_Memory_Utilized','RequestExecutionTime']]\n",
    "    return prediction \n",
    "\n",
    "# V1.2 : changed function name to be more descriptive and changed the date columns to just be assigned the min and max dates created earlier\n",
    "def formatsampledata(df, SampleStartDate, SampleEndDate, DateRange, tenant, role):\n",
    "    df['SampleStartDate']=SampleStartDate\n",
    "    df['SampleEndDate']=SampleEndDate\n",
    "    df['Tenant']=tenant.upper()\n",
    "    df['Role']=role\n",
    "    df['Region']='All'\n",
    "    df['CPU']=df['PercentProcessorTime']\n",
    "    df['MemoryAvailable']=df['AvailableMBytes']\n",
    "    df['MemoryUtilized']=df['Percent_Memory_Utilized']\n",
    "    df['RET']=df['RequestExecutionTime']\n",
    "    df['RPS']=df['Requests_sec']\n",
    "    df['DateRange']=DateRange\n",
    "    df=df.drop(['PercentProcessorTime','AvailableMBytes','Percent_Memory_Utilized','RequestExecutionTime','Requests_sec'],axis=1)\n",
    "    gc.collect()\n",
    "    df=df[['SampleStartDate','SampleEndDate','Tenant','Role','Region','DataCenter','CPU','MemoryAvailable','MemoryUtilized','RET','RPS','DateRange']]\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# V1.1 : instead of passing df(sample set), pass only the required date fields\n",
    "# V1.2 : (same as above)changed function name to be more descriptive and changed the date columns to just be assigned the min and max dates created earlier\n",
    "def formatpredictiondata(prediction,SampleStartDate,SampleEndDate,DateRange, tenant, role):\n",
    "    prediction['SampleStartDate']=SampleStartDate\n",
    "    prediction['SampleEndDate']=SampleEndDate\n",
    "    prediction['Tenant']=tenant.upper()\n",
    "    prediction['Role']=role\n",
    "    prediction['Region']='All'\n",
    "    prediction['FaultDomain']='All'\n",
    "    prediction['CPU']=prediction['PercentProcessorTime']\n",
    "    prediction['Memory']=prediction['Percent_Memory_Utilized']\n",
    "    prediction['RET']=prediction['RequestExecutionTime']\n",
    "    prediction['DateRange']=DateRange\n",
    "    today=datetime.date.today().strftime('%Y-%#m-%d')\n",
    "    prediction['PredictionDate']=today\n",
    "    prediction=prediction[['SampleStartDate','SampleEndDate','Tenant','Role','Region','FaultDomain','CPU','Memory','RET','RPS_MARS', 'RPS_LR','DateRange','PredictionDate']]\n",
    "    return prediction\n",
    "\n",
    "# V1.2 : changed name and now takes min and max dates as input variables\n",
    "def formatparameters (metrics, tenant, mindate, maxdate,role,path):\n",
    "    analysis_info=pd.DataFrame(metrics, columns=['parameter','value'])\n",
    "    analysis_info ['batch'] = tenant + \"_\" + role + \"_\" + maxdate\n",
    "    analysis_info['tenant'] = tenant\n",
    "    analysis_info['role'] = role\n",
    "    analysis_info['batch_start_date'] = mindate\n",
    "    analysis_info['batch_end_date'] = maxdate\n",
    "    analysis_info = analysis_info[['batch','tenant','role','batch_start_date','batch_end_date', 'parameter', 'value']]\n",
    "    return analysis_info\n",
    "\n",
    "def PowerBISample(df,No_PBI_samples):\n",
    "    df['CPU'] = round(df.PercentProcessorTime,1)\n",
    "    df['RPS'] = round(df.Requests_sec,0)\n",
    "    count = (df.groupby(['CPU','RPS']).count()).shape[0]\n",
    "    count = round(No_PBI_samples/count)\n",
    "    Power_BI_Sample = df.groupby(['CPU','RPS']).head(count)\n",
    "    metrics.append(['number of PowerBI sample rows', Power_BI_Sample.shape[0]])\n",
    "    metrics.append(['PowerBI dataset max', No_PBI_samples])\n",
    "    if (verbose):\n",
    "        print('power bi count',Power_BI_Sample.shape[0])\n",
    "    return Power_BI_Sample\n",
    "    \n",
    "#### DISTRIBUTION FUNCTIONS\n",
    "\n",
    "\n",
    "# V1.1 : Changed the distribution functions for memory optimization.\n",
    "def cpu_memory_feature_distribution (Data,mindate,maxdate,bucket,tenant,role,SampleType,feature):\n",
    "    if feature.lower() == 'cpu':\n",
    "        bucket=bucket\n",
    "        Data = Data[['PercentProcessorTime','TotalTransactions']]\n",
    "        Data['range_1'] = (Data['PercentProcessorTime'] // bucket) * bucket\n",
    "    if feature.lower() == 'memory':\n",
    "        if 'Percent_Memory_Utilized' not in Data.columns:\n",
    "            maxmemory = max(Data['AvailableMBytes'])\n",
    "            Data['Percent_Memory_Utilized'] = ((maxmemory - Data['AvailableMBytes'])/maxmemory)*100\n",
    "        bucket=bucket\n",
    "        Data = Data[['Percent_Memory_Utilized','TotalTransactions']]\n",
    "        Data['range_1'] = (Data['Percent_Memory_Utilized'] // bucket) * bucket\n",
    "    #memory_availability()\n",
    "    distribution = Data.groupby(['range_1'],as_index=False)['TotalTransactions'].agg({\"Count\": \"count\",\n",
    "                                                       \"TotalTransactions\" : \"sum\"})\n",
    "    #memory_availability()\n",
    "    Data=None\n",
    "    distribution = distribution.sort_values(by = 'range_1')\n",
    "    distribution=distribution.reset_index(drop=True)\n",
    "    distribution['range_2'] = np.where(distribution['range_1'] != 100,distribution['range_1'] + bucket ,distribution['range_1'])\n",
    "    distribution['range_1'] = distribution[['range_1']].astype('int').astype('str')\n",
    "    distribution['range_2'] = distribution[['range_2']].astype('int').astype('str')\n",
    "    distribution['Bucket'] = distribution['range_1'] + '-' + distribution['range_2']\n",
    "    nrecords = distribution[\"Count\"].sum()\n",
    "    totaltransactions = distribution[\"TotalTransactions\"].sum()\n",
    "    distribution['CountPercent'] = (distribution['Count']/nrecords)*100\n",
    "    distribution['TransactionsPercent'] = (distribution['TotalTransactions']/totaltransactions)*100\n",
    "    distribution['Tenant'] = tenant.upper()\n",
    "    distribution['Role'] = role\n",
    "    distribution['SampleStartDate'] = mindate\n",
    "    distribution['SampleEndDate'] = maxdate\n",
    "    distribution['SampleType'] = SampleType\n",
    "    distribution=distribution.drop(['range_1','range_2'], axis=1)\n",
    "    gc.collect()\n",
    "    distribution = distribution[['SampleStartDate','SampleEndDate','SampleType','Tenant','Role','Bucket','Count','CountPercent','TotalTransactions','TransactionsPercent']]\n",
    "    return distribution\n",
    "\n",
    "# V1.1 : Changed the distribution functions for memory optimization.\n",
    "def ret_rps_feature_distribution (Data,mindate,maxdate,tenant,role,SampleType,feature):\n",
    "    #memory_availability()\n",
    "    if feature.lower() == 'ret':\n",
    "        Data = Data[['RequestExecutionTime','TotalTransactions']]\n",
    "        Data = Data.sort_values(by=['RequestExecutionTime'],ascending=[True])\n",
    "        Data = Data.reset_index(drop=True)\n",
    "        Data['Percentile'] = pd.qcut(Data.index, 20, labels=False)\n",
    "        distribution = Data.groupby(['Percentile'],as_index=False)['TotalTransactions'].agg({\"Count\": \"count\",\n",
    "                                                           \"TotalTransactions\" : \"sum\"})\n",
    "        distribution2 = Data.groupby(['Percentile'],as_index=False)['RequestExecutionTime'].agg({\"range_1\": \"min\",\n",
    "                                                           \"range_2\" : \"max\"})\n",
    "        distribution = pd.merge(distribution,distribution2,how=\"inner\",on=['Percentile'])\n",
    "    if feature.lower() == 'rps':\n",
    "        Data = Data[['Requests_sec','TotalTransactions']]\n",
    "        Data = Data.sort_values(by=['Requests_sec'],ascending=[True])\n",
    "        Data = Data.reset_index(drop=True)\n",
    "        Data['Percentile'] = pd.qcut(Data.index, 20, labels=False)\n",
    "        distribution = Data.groupby(['Percentile'],as_index=False)['TotalTransactions'].agg({\"Count\": \"count\",\n",
    "                                                           \"TotalTransactions\" : \"sum\"})\n",
    "        distribution2 = Data.groupby(['Percentile'],as_index=False)['Requests_sec'].agg({\"range_1\": \"min\",\n",
    "                                                           \"range_2\" : \"max\"})\n",
    "        distribution = pd.merge(distribution,distribution2,how=\"inner\",on=['Percentile'])\n",
    "    #memory_availability()\n",
    "    Data=None\n",
    "    distribution = distribution.sort_values(by = 'range_1')\n",
    "    distribution['range_1'] = distribution[['range_1']].astype('str')\n",
    "    distribution['range_2'] = distribution[['range_2']].astype('str')\n",
    "    distribution['Bucket'] = distribution['range_1'] + '-' + distribution['range_2']\n",
    "    distribution['PercentileBucket'] = (distribution['Percentile']*5).astype('str') + '-' + (distribution['Percentile']*5+5).astype('str')\n",
    "    distribution=distribution.reset_index(drop=True)\n",
    "    nrecords = distribution[\"Count\"].sum()\n",
    "    totaltransactions = distribution[\"TotalTransactions\"].sum()\n",
    "    distribution['CountPercent'] = (distribution['Count']/nrecords)*100\n",
    "    distribution['TransactionsPercent'] = (distribution['TotalTransactions']/totaltransactions)*100\n",
    "    distribution['Tenant'] = tenant.upper()\n",
    "    distribution['Role'] = role\n",
    "    distribution['SampleStartDate'] = mindate\n",
    "    distribution['SampleEndDate'] = maxdate\n",
    "    distribution['SampleType'] = SampleType\n",
    "    distribution=distribution.drop(['range_1','range_2'], axis=1)\n",
    "    gc.collect()\n",
    "    distribution = distribution[['SampleStartDate','SampleEndDate','SampleType','Tenant','Role','Bucket','Count','CountPercent','TotalTransactions','TransactionsPercent','Percentile','PercentileBucket']]\n",
    "    return distribution    \n",
    "\n",
    "# V1.1 : Changed the distribution functions for memory optimization.\n",
    "# V1.1 : Added columns average RET and average Memory.\n",
    "def cpuvsrps_Distribution(Data,mindate,maxdate,cpuvsrps_rps_limit,cpuvsrps_rpsbucket,tenant,role,SampleType):\n",
    "    #print(\"cpurpsdist:\")\n",
    "    #memory_availability()\n",
    "    Data['CPU'] = round(Data['PercentProcessorTime'],0)\n",
    "    Data['RPS'] = (np.ceil(Data['Requests_sec']/cpuvsrps_rpsbucket)* cpuvsrps_rpsbucket) \n",
    "    Data=Data.drop(['PercentProcessorTime','Requests_sec'],axis=1)\n",
    "    gc.collect()\n",
    "    #memory_availability()\n",
    "    if cpuvsrps_rps_limit == None:\n",
    "        cpuvsrps_rps_limit = (np.ceil(max(Data['RPS'])/cpuvsrps_rpsbucket)* cpuvsrps_rpsbucket) - cpuvsrps_rpsbucket\n",
    "    #memory_availability()\n",
    "    Data['RPS'] = np.where((Data['RPS'] > cpuvsrps_rps_limit),cpuvsrps_rps_limit + cpuvsrps_rpsbucket ,Data['RPS'])\n",
    "    CPU_RPS_distribution = Data.groupby(['CPU','RPS'],as_index=False)['TotalTransactions'].agg({\"Count\": \"count\",\"TotalTransactions\" : \"sum\"})\n",
    "    CPU_RPS_distribution2 = Data.groupby(['CPU','RPS'],as_index=False)['Percent_Memory_Utilized'].agg({\"AverageMemory\" : \"mean\"})\n",
    "    CPU_RPS_distribution3 = Data.groupby(['CPU','RPS'],as_index=False)['RequestExecutionTime'].agg({\"AverageRET\" : \"mean\"})\n",
    "    Data=None\n",
    "    gc.collect()\n",
    "    CPU_RPS_distribution = pd.merge(CPU_RPS_distribution,CPU_RPS_distribution2,how=\"inner\",on=['CPU','RPS'])\n",
    "    CPU_RPS_distribution2 = None\n",
    "    gc.collect()\n",
    "    CPU_RPS_distribution = pd.merge(CPU_RPS_distribution,CPU_RPS_distribution3,how=\"inner\",on=['CPU','RPS'])\n",
    "    CPU_RPS_distribution3 = None\n",
    "    gc.collect()\n",
    "    #memory_availability()\n",
    "    CPU_RPS_distribution['CPU'] = CPU_RPS_distribution['CPU'].astype('int')\n",
    "    CPU_RPS_distribution['RPS'] = CPU_RPS_distribution['RPS'].astype('int')\n",
    "    CPU_RPS_distribution['CumulativeCount'] = CPU_RPS_distribution.groupby(['CPU'])['Count'].apply(lambda x: x.cumsum())\n",
    "    CPU_RPS_distribution['TotalCount'] = CPU_RPS_distribution.groupby(['CPU'])['Count'].transform('sum')\n",
    "    CPU_RPS_distribution['Percent'] = (CPU_RPS_distribution['CumulativeCount']/CPU_RPS_distribution['TotalCount'])*100\n",
    "    CPU_RPS_distribution['RPSDescription'] = np.where((CPU_RPS_distribution['RPS'] > cpuvsrps_rps_limit),(\"> \" + ((CPU_RPS_distribution['RPS']-cpuvsrps_rpsbucket).astype('str'))),(\"<= \" + (CPU_RPS_distribution['RPS'].astype('str'))))\n",
    "    CPU_RPS_distribution['RPSRange'] = np.where((CPU_RPS_distribution['RPS'] > cpuvsrps_rps_limit),(\"> \" + ((CPU_RPS_distribution['RPS']-cpuvsrps_rpsbucket).astype('str'))),(np.maximum((CPU_RPS_distribution['RPS'] - cpuvsrps_rpsbucket).astype('int'),0).astype('str') + \" - \" + (CPU_RPS_distribution['RPS'].astype('str'))))\n",
    "    CPU_RPS_distribution['Tenant'] = tenant.upper()\n",
    "    CPU_RPS_distribution['Role'] = role\n",
    "    CPU_RPS_distribution['SampleStartDate'] = mindate\n",
    "    CPU_RPS_distribution['SampleEndDate'] = maxdate\n",
    "    CPU_RPS_distribution['SampleType'] = SampleType\n",
    "    gc.collect()\n",
    "    CPU_RPS_distribution = CPU_RPS_distribution[['SampleStartDate','SampleEndDate','SampleType','Tenant','Role','CPU','RPS','Count','CumulativeCount','TotalCount','Percent','AverageMemory','AverageRET','TotalTransactions']]\n",
    "    return(CPU_RPS_distribution)\n",
    "\n",
    "# function to push distributions into blob\n",
    "def push_distributions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,df,SampleType,feature):\n",
    "    blobpredictionfilepattern=\"{:#tenant#/efficiencyview/%Y/%m/distributions/#feature#/#tenant#_#role#_efficiencyview_#feature#_distribution_#SampleType#_%Y%m.csv}\"\n",
    "    blobservice = BlockBlobService(storageaccountname,accountkey)\n",
    "    blobfile= blobpredictionfilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant).replace('#feature#', feature.lower()).replace('#SampleType#', SampleType.lower())\n",
    "    data= StringIO()\n",
    "    df.to_csv(data, index=False)\n",
    "    data=bytes(data.getvalue(), 'utf-8')\n",
    "    data=BytesIO(data)\n",
    "    blobservice.create_blob_from_stream(container,blobfile,data)\n",
    "    data.close()\n",
    "    if (savelocally):\n",
    "        localpredictionfilepattern=\"{:#tenant#_#role#_efficiencyview_#feature#_distribution_#SampleType#_%Y%m.csv}\"\n",
    "        localfile= localpredictionfilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant).replace('#feature#', feature.lower()).replace('#SampleType#', SampleType.lower())\n",
    "        df.to_csv(os.path.join(path,localfile), index=False)\n",
    "\n",
    "# V1.2 : put the dist_savefiles function within this distributions function so that only one function needs to be called in the efficiency view\n",
    "def distributions(Data,mindate,maxdate,tenant,role,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit,path,asofdate,SampleType):\n",
    "    if(cpu_bucket == None):\n",
    "        cpu_bucket=5\n",
    "    if(memory_bucket == None):\n",
    "        memory_bucket=5\n",
    "    if(cpuvsrps_rpsbucket == None):\n",
    "        cpuvsrps_rpsbucket=5\n",
    "    gc.collect()\n",
    "    ret_dist = ret_rps_feature_distribution(Data,mindate,maxdate,tenant,role,SampleType,feature='ret')\n",
    "    rps_dist = ret_rps_feature_distribution(Data,mindate,maxdate,tenant,role,SampleType,feature='rps')\n",
    "    cpu_dist = cpu_memory_feature_distribution(Data,mindate,maxdate,cpu_bucket,tenant,role,SampleType,feature='cpu')\n",
    "    memory_dist = cpu_memory_feature_distribution(Data,mindate,maxdate,cpu_bucket,tenant,role,SampleType,feature='memory')\n",
    "    cpurps_dist = cpuvsrps_Distribution(Data,mindate,maxdate,cpuvsrps_rps_limit,cpuvsrps_rpsbucket,tenant,role,SampleType)\n",
    "    push_distributions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,ret_dist,SampleType,feature='ret')\n",
    "    push_distributions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,rps_dist,SampleType,feature='rps')\n",
    "    push_distributions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,cpu_dist,SampleType,feature='cpu')\n",
    "    push_distributions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,memory_dist,SampleType,feature='memory')\n",
    "    push_distributions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,cpurps_dist,SampleType,feature='cpurps')\n",
    "    gc.collect()\n",
    "\n",
    "# function to push quantiles to the blob\n",
    "def push_quantiles_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,df,SampleType):\n",
    "    blobsamplefilepattern=\"{:#tenant#/efficiencyview/%Y/%m/quantiles/quantiles/#tenant#_#role#_efficiencyview_quantiles_#SampleType#_%Y%m.csv}\"\n",
    "    blobservice = BlockBlobService(storageaccountname,accountkey)\n",
    "    blobfile= blobsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant).replace('#SampleType#', SampleType.lower())\n",
    "    data= StringIO()\n",
    "    df.to_csv(data, index=False)\n",
    "    data=bytes(data.getvalue(), 'utf-8')\n",
    "    data=BytesIO(data)\n",
    "    blobservice.create_blob_from_stream(container,blobfile,data)\n",
    "    data.close()\n",
    "    if (savelocally):\n",
    "        localsamplefilepattern=\"{:#tenant#_#role#_efficiencyview_quantiles_#SampleType#_%Y%m.csv}\"\n",
    "        localfile= localsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant).replace('#SampleType#', SampleType.lower())\n",
    "        df.to_csv(os.path.join(path,localfile), index=False)\n",
    "\n",
    "# function to push summarystats dataframe into blob\n",
    "def push_summarystats_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,df,SampleType):\n",
    "    blobsamplefilepattern=\"{:#tenant#/efficiencyview/%Y/%m/quantiles/summarystats/#tenant#_#role#_efficiencyview_summarystats_#SampleType#_%Y%m.csv}\"\n",
    "    blobservice = BlockBlobService(storageaccountname,accountkey)\n",
    "    blobfile= blobsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant).replace('#SampleType#', SampleType.lower())\n",
    "    data= StringIO()\n",
    "    df.to_csv(data, index=False)\n",
    "    data=bytes(data.getvalue(), 'utf-8')\n",
    "    data=BytesIO(data)\n",
    "    blobservice.create_blob_from_stream(container,blobfile,data)\n",
    "    data.close()\n",
    "    if (savelocally):\n",
    "        localsamplefilepattern=\"{:#tenant#_#role#_efficiencyview_summarystats_#SampleType#_%Y%m.csv}\"\n",
    "        localfile= localsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant).replace('#SampleType#', SampleType.lower())\n",
    "        df.to_csv(os.path.join(path,localfile), index=False)        \n",
    "\n",
    "# V1.2 : put the quantiles_savefiles function within this quantilesandsummary function so that only one function needs to be called in the efficiency view\n",
    "def quantilesandsummary (Data,mindate,maxdate,tenant,role,path,asofdate,quantile_i,SampleType):\n",
    "    quantile_i = quantile_i/100\n",
    "    quants = np.arange(0,1.0001,quantile_i)\n",
    "    Quantiles = pd.DataFrame({'quantile' : quants ,'RequestExecutionTime': [Data['RequestExecutionTime'].quantile(x) for x in quants],\n",
    "             'PercentProcessorTime': [Data['PercentProcessorTime'].quantile(x) for x in quants],\n",
    "             'Requests_sec': [Data['Requests_sec'].quantile(x) for x in quants],\n",
    "             'Percent_Memory_Utilized': [Data['Percent_Memory_Utilized'].quantile(x) for x in quants]})\n",
    "    \n",
    "    Quantiles['Tenant'] = tenant.upper()\n",
    "    Quantiles['Role'] = role\n",
    "    Quantiles['SampleStartDate'] = mindate\n",
    "    Quantiles['SampleEndDate'] = maxdate\n",
    "    Quantiles['SampleType'] = SampleType\n",
    "    Metrics = ['Standard deviation','mean','median','min','max']\n",
    "    summarystats = pd.DataFrame({'Metrics' : Metrics ,'RequestExecutionTime': [Data.RequestExecutionTime.std(),Data.RequestExecutionTime.mean(),Data.RequestExecutionTime.median(),Data.RequestExecutionTime.min(),Data.RequestExecutionTime.max()],\n",
    "             'PercentProcessorTime': [Data.PercentProcessorTime.std(),Data.PercentProcessorTime.mean(),Data.PercentProcessorTime.median(),Data.PercentProcessorTime.min(),Data.PercentProcessorTime.max()],\n",
    "             'Requests_sec': [Data.Requests_sec.std(),Data.Requests_sec.mean(),Data.Requests_sec.median(),Data.Requests_sec.min(),Data.Requests_sec.max()],\n",
    "             'Percent_Memory_Utilized': [Data.Percent_Memory_Utilized.std(),Data.Percent_Memory_Utilized.mean(),Data.Percent_Memory_Utilized.median(),Data.Percent_Memory_Utilized.min(),Data.Percent_Memory_Utilized.max()]})\n",
    "    \n",
    "    summarystats['Tenant'] = tenant.upper()\n",
    "    summarystats['Role'] = role\n",
    "    summarystats['SampleStartDate'] = mindate\n",
    "    summarystats['SampleEndDate'] = maxdate\n",
    "    summarystats['SampleType'] = SampleType\n",
    "    push_quantiles_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,Quantiles,SampleType)\n",
    "    push_summarystats_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,summarystats,SampleType)\n",
    "    gc.collect()\n",
    "\n",
    "# function to push prediction set dataframe into blob\n",
    "def push_pbiprediction_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction):\n",
    "    blobpredictionfilepattern=\"{:#tenant#/efficiencyview/%Y/%m/predictions/#tenant#_#role#_efficiencyview_predictionset_%Y%m.csv}\"\n",
    "    blobservice = BlockBlobService(storageaccountname,accountkey)\n",
    "    blobfile= blobpredictionfilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "    if role.lower() in ['estsfe', 'restdirectoryservice', 'adminwebservice']:\n",
    "        prediction = prediction.drop('RPS_LR', axis=1)\n",
    "        prediction.rename(columns={'RPS_MARS':'RPS'}, inplace=True)\n",
    "    else:\n",
    "        prediction = prediction.drop('RPS_MARS', axis=1)\n",
    "        prediction.rename(columns={'RPS_LR':'RPS'}, inplace=True)\n",
    "    data= StringIO()\n",
    "    prediction.to_csv(data, index=False)\n",
    "    data=bytes(data.getvalue(), 'utf-8')\n",
    "    data=BytesIO(data)\n",
    "    blobservice.create_blob_from_stream(container,blobfile,data)\n",
    "    data.close()\n",
    "    if(savelocally):\n",
    "        localpredictionfilepattern=\"{:#tenant#_#role#_efficiencyview_predictionset_%Y%m.csv}\"\n",
    "        localfile= localpredictionfilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "        prediction.to_csv(os.path.join(path,localfile), index=False)\n",
    "\n",
    "# function to push sample set dataframe into blob\n",
    "def push_pbisample_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample):\n",
    "    blobsamplefilepattern=\"{:#tenant#/efficiencyview/%Y/%m/sampleset/#tenant#_#role#_efficiencyview_sampleset_%Y%m.csv}\"\n",
    "    blobservice = BlockBlobService(storageaccountname,accountkey)\n",
    "    blobfile= blobsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "    data= StringIO()\n",
    "    sample.to_csv(data, index=False)\n",
    "    data=bytes(data.getvalue(), 'utf-8')\n",
    "    data=BytesIO(data)\n",
    "    blobservice.create_blob_from_stream(container,blobfile,data)\n",
    "    data.close()\n",
    "    if (savelocally):\n",
    "        localsamplefilepattern=\"{:#tenant#_#role#_efficiencyview_sampleset_%Y%m.csv}\"\n",
    "        localfile= localsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "        sample.to_csv(os.path.join(path,localfile), index=False)\n",
    "        \n",
    "# function to push training data dataframe into blob\n",
    "def push_trainingdata_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample):\n",
    "    blobsamplefilepattern=\"{:#tenant#/efficiencyview/%Y/%m/trainingdata/#tenant#_#role#_efficiencyview_trainingdata_%Y%m.csv}\"\n",
    "    blobservice = BlockBlobService(storageaccountname,accountkey)\n",
    "    blobfile= blobsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "    data= StringIO()\n",
    "    sample.to_csv(data, index=False)\n",
    "    data=bytes(data.getvalue(), 'utf-8')\n",
    "    data=BytesIO(data)\n",
    "    blobservice.create_blob_from_stream(container,blobfile,data)\n",
    "    data.close()\n",
    "    if (savelocally):\n",
    "        localsamplefilepattern=\"{:#tenant#_#role#_efficiencyview_trainingdata_%Y%m.csv}\"\n",
    "        localfile= localsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "        sample.to_csv(os.path.join(path,localfile), index=False)\n",
    "\n",
    "# function to push metrics dataframe into blob\n",
    "def push_metrics_toblob(storageacountname,accountkey,tenant,role,container,asofdate,metrics):\n",
    "    blobsamplefilepattern=\"{:#tenant#/efficiencyview/%Y/%m/metrics/#tenant#_#role#_efficiencyview_metrics_%Y%m.csv}\"\n",
    "    blobservice = BlockBlobService(storageacountname,accountkey)\n",
    "    blobfile= blobsamplefilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "    data= StringIO()\n",
    "    metrics.to_csv(data, index=False)\n",
    "    data=bytes(data.getvalue(), 'utf-8')\n",
    "    data=BytesIO(data)\n",
    "    blobservice.create_blob_from_stream(container,blobfile,data)\n",
    "    data.close()\n",
    "    if (savelocally):\n",
    "        localmetricsfilepattern=\"{:#tenant#_#role#_efficiencyview_metrics_%Y%m.csv}\"\n",
    "        localfile= localmetricsfilepattern.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "        metrics.to_csv(os.path.join(path,localfile), index=False)\n",
    "    \n",
    "# function to push both prediction set dataframes into blob\n",
    "def push_bothpredictions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction):\n",
    "    blobservice = BlockBlobService(storageaccountname,accountkey)\n",
    "    #mars prediction\n",
    "    prediction_mars=prediction\n",
    "    prediction_mars = prediction_mars.drop('RPS_LR', axis=1)\n",
    "    prediction_mars.rename(columns={'RPS_MARS':'RPS'}, inplace=True)\n",
    "    prediction_mars['ModelType']='MARS'\n",
    "    blobpredictionfilepattern_mars=\"{:#tenant#/efficiencyview/%Y/%m/mars_lr_predictions/#tenant#_#role#_efficiencyview_predictionset_mars_%Y%m.csv}\"\n",
    "    blobfile_mars= blobpredictionfilepattern_mars.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "    data_mars= StringIO()\n",
    "    prediction_mars.to_csv(data_mars, index=False)\n",
    "    data_mars=bytes(data_mars.getvalue(), 'utf-8')\n",
    "    data_mars=BytesIO(data_mars)\n",
    "    blobservice.create_blob_from_stream(container,blobfile_mars,data_mars)\n",
    "    data_mars.close()\n",
    "    #lr prediction\n",
    "    prediction_lr=prediction\n",
    "    prediction_lr = prediction_lr.drop('RPS_MARS', axis=1)\n",
    "    prediction_lr.rename(columns={'RPS_LR':'RPS'}, inplace=True)\n",
    "    prediction_lr['ModelType']='LR'\n",
    "    blobpredictionfilepattern_lr=\"{:#tenant#/efficiencyview/%Y/%m/mars_lr_predictions/#tenant#_#role#_efficiencyview_predictionset_lr_%Y%m.csv}\"\n",
    "    blobfile_lr= blobpredictionfilepattern_lr.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "    data_lr= StringIO()\n",
    "    prediction_lr.to_csv(data_lr, index=False)\n",
    "    data_lr=bytes(data_lr.getvalue(), 'utf-8')\n",
    "    data_lr=BytesIO(data_lr)\n",
    "    blobservice.create_blob_from_stream(container,blobfile_lr,data_lr)\n",
    "    data_lr.close()\n",
    "    if (savelocally):\n",
    "        localpredictionfilepattern_mars=\"{:#tenant#_#role#_efficiencyview_predictionset_mars_%Y%m.csv}\"\n",
    "        localfile_mars= localpredictionfilepattern_mars.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "        prediction_mars.to_csv(os.path.join(path,localfile_mars), index=False)\n",
    "        localpredictionfilepattern_lr=\"{:#tenant#_#role#_efficiencyview_predictionset_lr_%Y%m.csv}\"\n",
    "        localfile_lr= localpredictionfilepattern_lr.format(asofdate).replace('#role#',role).replace('#tenant#',tenant)\n",
    "        prediction_lr.to_csv(os.path.join(path,localfile_lr), index=False)\n",
    "    \n",
    "def memory_availability():\n",
    "    values = psutil.virtual_memory()\n",
    "    total = values.total /(1024.0 ** 3)\n",
    "    used = values.used /(1024.0 ** 3)\n",
    "    available = values.available /(1024.0 ** 3)\n",
    "    print(\"Total memory = {}; used memory = {}; available memory = {}\".format(total, used,available))\n",
    "\n",
    "# V1.1 : Added new function parameters.\n",
    "# V1.1 : Calling the getinputfilefrom blob in this method : saves memory\n",
    "# V1.2 : Cleaned efficiency function so that most processes occur within functions\n",
    "def efficiency(tenant,role,asofdate,storageaccountname,accountkey,container,No_PBI_samples,Sample_Percent,random_State,cv,cpu_bucket=None,memory_bucket=None,cpuvsrps_rpsbucket=None,cpuvsrps_rps_limit=None):\n",
    "# Read data from blob\n",
    "    print(\"Opening memory : \")\n",
    "    memory_availability()\n",
    "    path = makedirectory(tenant,role,asofdate)\n",
    "    #for dev use below\n",
    "    #container2='data'\n",
    "    #storageaccountname2= 'identitytrafficstorage'\n",
    "    #accountkey2= 'SAvw58YxX+8C0lO3uS26Ux3wIWcRE/zzjq6kqHJBhLXAcYa8vlhsCt7+p5NULjlcjEynYY1ZmkE3ecNrM5fu6Q=='\n",
    "    #df = getinputfile_fromblob(storageaccountname2,accountkey2,container2,tenant,role,asofdate)\n",
    "    #for prod use below\n",
    "    df = getinputfile_fromblob(storageaccountname,accountkey,container,tenant,role,asofdate)\n",
    "    #for gateway-auth\n",
    "    #df = df[~df['Day'].isin(['3/1/2018','3/2/2018','3/3/2018','3/4/2018','3/5/2018'])]\n",
    "    rawdatametrics(df)\n",
    "    gc.collect()\n",
    "    print(\"Memory after loading data : \")\n",
    "    memory_availability()\n",
    "# Generate date variables\n",
    "    df=df.drop(['Tenant','Role'], axis=1)\n",
    "    mindate, maxdate, month, year, daterange=dates(df)\n",
    "    gc.collect()\n",
    "    print(\"Memory after generating dates : \")\n",
    "    memory_availability()\n",
    "# Generate new variables\n",
    "    df=addmemoryutilized(df)\n",
    "    gc.collect()\n",
    "# Split data\n",
    "    df2 = df[['DataCenter','AvailableMBytes']]\n",
    "    df = df[['PercentProcessorTime', 'Requests_sec','RequestExecutionTime','Percent_Memory_Utilized','TotalTransactions']]\n",
    "# Generate distribution and quantile files for active server data\n",
    "    #distributions(df,mindate,maxdate,tenant,role,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit,path,asofdate,SampleType='ActiveServerData')\n",
    "    #gc.collect()\n",
    "    #quantilesandsummary(df,mindate,maxdate,tenant,role,path,asofdate,quantile_i=5,SampleType='ActiveServerData')\n",
    "    #gc.collect()\n",
    "    #print(\"Memory after generating server distribution data : \")\n",
    "    #memory_availability()\n",
    "# Remove outliers and filter by influence\n",
    "    df = filterbySTD(df)\n",
    "    gc.collect()\n",
    "    print(\"Memory after filtering by standard deviation : \")\n",
    "    memory_availability()\n",
    "    df,sampletestdf = filterbyInfluence(df,random_State,Sample_Percent,path)\n",
    "    gc.collect()\n",
    "    print(\"Memory after filtering by influence and sampling : \")\n",
    "    memory_availability()\n",
    "# Generate distribution data for sample train data\n",
    "    #distributions(df,mindate,maxdate,tenant,role,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit,path,asofdate,SampleType='SampleTrainData')\n",
    "    #gc.collect()\n",
    "    #quantilesandsummary(df,mindate,maxdate,tenant,role,path,asofdate,quantile_i=5,SampleType='SampleTrainData')\n",
    "    #gc.collect()\n",
    "    #print(\"Memory after generating sample distribution data : \")\n",
    "    #memory_availability()\n",
    "# Generate prediction data\n",
    "    prediction = generateprediction(df)\n",
    "    gc.collect()\n",
    "    print(\"Memory after building prediction data : \")\n",
    "    memory_availability()\n",
    "# Build the model\n",
    "#    if role.lower() in ['estsfe', 'restdirectoryservice', 'adminwebservice']:\n",
    "#        prediction = mars(df,sampletestdf, random_State,path, prediction)\n",
    "#        gc.collect()\n",
    "#        print(\"Memory after building the model : \")\n",
    "#        memory_availability()\n",
    "#    else:\n",
    "#        prediction = linear_regression(df,sampletestdf, random_State,path,prediction,cv=5)\n",
    "#        gc.collect()\n",
    "#        print(\"Memory after building the model : \")\n",
    "#        memory_availability()\n",
    "    prediction = comparemodels(df, sampletestdf, prediction, mindate, maxdate, tenant, role, random_State, path, cv)\n",
    "    gc.collect()\n",
    "    print(\"Memory after building the model : \")\n",
    "    memory_availability()\n",
    "# Get parameters\n",
    "    parameters = formatparameters(metrics, tenant, mindate, maxdate,role,path)\n",
    "    gc.collect()\n",
    "    print(\"Memory after saving parameters : \")\n",
    "    memory_availability()\n",
    "# Collect sample train data\n",
    "    sampletestdf = None\n",
    "    gc.collect()\n",
    "    df = pd.merge(df,df2,left_index=True,right_index=True,how='inner')\n",
    "    gc.collect()\n",
    "    print(\"Memory after fetching train data : \")\n",
    "    memory_availability()\n",
    "# Generate power BI sample\n",
    "    Powerbi_sample = PowerBISample(df, No_PBI_samples)\n",
    "    gc.collect()\n",
    "    print(\"Memory after generating power bi sample : \")\n",
    "    memory_availability()\n",
    "# Format the files for power BI\n",
    "    Powerbi_sample = formatsampledata(Powerbi_sample, mindate,maxdate,daterange, tenant, role)\n",
    "    df = formatsampledata(df,mindate,maxdate,daterange, tenant, role)\n",
    "    gc.collect()\n",
    "    prediction= formatpredictiondata(prediction,mindate,maxdate,daterange, tenant, role)\n",
    "    gc.collect()\n",
    "    print(\"closing memory : \")\n",
    "    memory_availability()\n",
    "    return df,Powerbi_sample,prediction,parameters,path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-19 22:23:11.479866\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "import datetime\n",
    "print(str(datetime.datetime.today()))\n",
    "asofdate=datetime.datetime(2018, 3, 31,0,0,0,0)\n",
    "cv=5\n",
    "No_PBI_samples=700000\n",
    "Sample_Percent=0.7\n",
    "random_State=10\n",
    "cpu_bucket=5\n",
    "memory_bucket=5\n",
    "cpuvsrps_rpsbucket=5\n",
    "cpuvsrps_rps_limit=100\n",
    "\n",
    "#prod details\n",
    "container='data'\n",
    "storageaccountname= 'identitytrafficstorage'\n",
    "accountkey= 'SAvw58YxX+8C0lO3uS26Ux3wIWcRE/zzjq6kqHJBhLXAcYa8vlhsCt7+p5NULjlcjEynYY1ZmkE3ecNrM5fu6Q=='\n",
    "\n",
    "#dev details\n",
    "#container='data'\n",
    "#storageaccountname= 'devidtrafficstorage'\n",
    "#accountkey= '7LjK/HqpmpSffbsi03TXSw8Jv4F9OWMQ47/I3I5Trd8rvUd3N1lyHOapBG/WrL3mtj5Gmvpeo/0wUoT49HFOvQ=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDODSV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(str(datetime.datetime.today()))\n",
    "tenant='msodsv2'\n",
    "msodsv2roles = ['directoryproxy']#['becwebservice','adminwebservice','companymanager','msods-syncservice','restdirectoryservice','directoryproxy']\n",
    "for i in msodsv2roles:\n",
    "    print(str(datetime.datetime.today()))\n",
    "    print(\"Producing Efficiency View for : \" , i)\n",
    "    role = i\n",
    "    metrics=[]\n",
    "    sample,Powerbi_sample,prediction,parameters,path = efficiency(tenant,role,asofdate,storageaccountname,accountkey,container,No_PBI_samples,Sample_Percent,random_State,cv,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit)\n",
    "    #push_bothpredictions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    #push_pbiprediction_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    #push_pbisample_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,Powerbi_sample)\n",
    "    #push_trainingdata_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample)\n",
    "    push_metrics_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,parameters)\n",
    "    gc.collect()\n",
    "    print(str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(str(datetime.datetime.today()))\n",
    "tenant='msgraph'\n",
    "msgraphroles = ['AGSFE']\n",
    "for i in msgraphroles:\n",
    "    print(\"Producing Efficiency View for : \" , i)\n",
    "    role = i\n",
    "    metrics=[]\n",
    "    sample,Powerbi_sample,prediction,parameters,path = efficiency(tenant,role,asofdate,storageaccountname,accountkey,container,No_PBI_samples,Sample_Percent,random_State,cv,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit)\n",
    "    push_bothpredictions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbiprediction_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbisample_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,Powerbi_sample)\n",
    "    push_trainingdata_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample)\n",
    "    push_metrics_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,parameters)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(str(datetime.datetime.today()))\n",
    "tenant='ests'\n",
    "estsroles = ['ESTSFE']\n",
    "for i in estsroles:\n",
    "    print(\"Producing Efficiency View for : \" , i)\n",
    "    role = i\n",
    "    metrics=[]\n",
    "    sample,Powerbi_sample,prediction,parameters,path = efficiency(tenant,role,asofdate,storageaccountname,accountkey,container,No_PBI_samples,Sample_Percent,random_State,cv,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit)\n",
    "    push_bothpredictions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    #push_pbiprediction_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbisample_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,Powerbi_sample)\n",
    "    push_trainingdata_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample)\n",
    "    push_metrics_toblob(storageacountname,accountkey,tenant,role,container,asofdate,parameters)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 16:29:49.282453\n",
      "Producing Efficiency View for :  gateway-auth\n",
      "Opening memory : \n",
      "Total memory = 127.99956130981445; used memory = 18.252113342285156; available memory = 109.7474479675293\n",
      "Memory after loading data : \n",
      "Total memory = 127.99956130981445; used memory = 33.80404281616211; available memory = 94.19551849365234\n",
      "Memory after generating dates : \n",
      "Total memory = 127.99956130981445; used memory = 31.777774810791016; available memory = 96.22178649902344\n",
      "% of Data between standard deviation range -1 & 1 : 0.6614726406238697\n",
      "% of Data between standard deviation range -2 & 2 : 0.9482523704774488\n",
      "% of Data between standard deviation range -3 & 3 : 0.997002122625989\n",
      "Number of rows after cleaning data: 134339060\n",
      "Memory after filtering by standard deviation : \n",
      "Total memory = 127.99956130981445; used memory = 27.608821868896484; available memory = 100.39073944091797\n",
      "Number of rows after filtering by influence: 134339048\n",
      "sample count 94037329\n",
      "Memory after filtering by influence and sampling : \n",
      "Total memory = 127.99956130981445; used memory = 27.638256072998047; available memory = 100.3613052368164\n",
      "RET_lower: 3 RET_upper: 5 RET_step: 1\n",
      "RAM_lower: 25 RAM_upper: 35 RAM_step: 1\n",
      "Memory after building prediction data : \n",
      "Total memory = 127.99956130981445; used memory = 27.64133071899414; available memory = 100.35823059082031\n",
      "Fitting MARS model\n",
      "Earth Model\n",
      "---------------------------------------------------------\n",
      "Basis Function                      Pruned  Coefficient  \n",
      "---------------------------------------------------------\n",
      "(Intercept)                         No      45.9303      \n",
      "h(PercentProcessorTime-43.5806)     No      -31.7037     \n",
      "h(43.5806-PercentProcessorTime)     No      -1.33832     \n",
      "h(RequestExecutionTime-8.951)       No      0.272448     \n",
      "h(8.951-RequestExecutionTime)       No      2.83508      \n",
      "h(PercentProcessorTime-44.2997)     No      31.6626      \n",
      "h(44.2997-PercentProcessorTime)     Yes     None         \n",
      "h(Percent_Memory_Utilized-70.0175)  No      -0.880358    \n",
      "h(70.0175-Percent_Memory_Utilized)  No      -0.090468    \n",
      "h(RequestExecutionTime-41.3215)     No      -0.299049    \n",
      "h(41.3215-RequestExecutionTime)     Yes     None         \n",
      "---------------------------------------------------------\n",
      "MSE: 9.1247, GCV: 9.1247, RSQ: 0.8713, GRSQ: 0.8713\n",
      "Fitting LR model\n",
      "cross val mse: 14.935125\n",
      "R2: 0.79\n",
      "mean absoluter error: 2.729961\n",
      "Mean squared error: 14.62\n",
      "Memory after building the model : \n",
      "Total memory = 127.99956130981445; used memory = 30.177711486816406; available memory = 97.82184982299805\n",
      "Memory after saving parameters : \n",
      "Total memory = 127.99956130981445; used memory = 30.33432388305664; available memory = 97.66523742675781\n",
      "Memory after fetching train data : \n",
      "Total memory = 127.99956130981445; used memory = 35.40081787109375; available memory = 92.5987434387207\n",
      "power bi count 417419\n",
      "Memory after generating power bi sample : \n",
      "Total memory = 127.99956130981445; used memory = 39.15279006958008; available memory = 88.84677124023438\n",
      "closing memory : \n",
      "Total memory = 127.99956130981445; used memory = 43.57734298706055; available memory = 84.4222183227539\n"
     ]
    }
   ],
   "source": [
    "print(str(datetime.datetime.today()))\n",
    "tenant='gateway'\n",
    "gatewayroles = ['gateway-auth']#, 'gateway-nonauth']\n",
    "for i in gatewayroles:\n",
    "    print(\"Producing Efficiency View for : \" , i)\n",
    "    role = i\n",
    "    metrics=[]\n",
    "    sample,Powerbi_sample,prediction,parameters,path = efficiency(tenant,role,asofdate,storageaccountname,accountkey,container,No_PBI_samples,Sample_Percent,random_State,cv,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit)\n",
    "    #push_bothpredictions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    #push_pbiprediction_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    #push_pbisample_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,Powerbi_sample)\n",
    "    #push_trainingdata_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample)\n",
    "    push_metrics_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,parameters)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-20 00:36:44.265679\n",
      "Producing Efficiency View for :  ADDataService\n",
      "Opening memory : \n",
      "Total memory = 127.99956130981445; used memory = 44.82783889770508; available memory = 83.17172241210938\n",
      "Memory after loading data : \n",
      "Total memory = 127.99956130981445; used memory = 44.983726501464844; available memory = 83.01583480834961\n",
      "Memory after generating dates : \n",
      "Total memory = 127.99956130981445; used memory = 44.95704650878906; available memory = 83.04251480102539\n",
      "% of Data between standard deviation range -1 & 1 : 0.7261885413754771\n",
      "% of Data between standard deviation range -2 & 2 : 0.9388203757998912\n",
      "% of Data between standard deviation range -3 & 3 : 0.9865695203794838\n",
      "Number of rows after cleaning data: 1821085\n",
      "Memory after filtering by standard deviation : \n",
      "Total memory = 127.99956130981445; used memory = 44.938262939453125; available memory = 83.06129837036133\n",
      "Number of rows after filtering by influence: 1821085\n",
      "sample count 1274759\n",
      "Memory after filtering by influence and sampling : \n",
      "Total memory = 127.99956130981445; used memory = 44.94752883911133; available memory = 83.05203247070312\n",
      "RET_lower: 545 RET_upper: 2437 RET_step: 189\n",
      "RAM_lower: 36 RAM_upper: 54 RAM_step: 2\n",
      "Memory after building prediction data : \n",
      "Total memory = 127.99956130981445; used memory = 44.94754409790039; available memory = 83.05201721191406\n",
      "Fitting MARS model\n",
      "Earth Model\n",
      "-------------------------------------------------------\n",
      "Basis Function                   Pruned  Coefficient   \n",
      "-------------------------------------------------------\n",
      "(Intercept)                      No      5.01974       \n",
      "h(PercentProcessorTime-17.1191)  Yes     None          \n",
      "h(17.1191-PercentProcessorTime)  No      -0.114117     \n",
      "h(RequestExecutionTime-2788.49)  No      0.00175551    \n",
      "h(2788.49-RequestExecutionTime)  No      -0.00155906   \n",
      "h(RequestExecutionTime-920.868)  No      -0.000193862  \n",
      "h(920.868-RequestExecutionTime)  Yes     None          \n",
      "h(PercentProcessorTime-35.5752)  No      0.00454743    \n",
      "h(35.5752-PercentProcessorTime)  No      0.0328875     \n",
      "h(RequestExecutionTime-163.088)  No      -0.00154984   \n",
      "h(163.088-RequestExecutionTime)  Yes     None          \n",
      "-------------------------------------------------------\n",
      "MSE: 0.0802, GCV: 0.0802, RSQ: 0.5923, GRSQ: 0.5923\n",
      "Fitting LR model\n",
      "cross val mse: 0.118490\n",
      "R2: 0.40\n",
      "mean absoluter error: 0.238465\n",
      "Mean squared error: 0.12\n",
      "Memory after building the model : \n",
      "Total memory = 127.99956130981445; used memory = 44.97285842895508; available memory = 83.02670288085938\n",
      "Memory after saving parameters : \n",
      "Total memory = 127.99956130981445; used memory = 44.97285842895508; available memory = 83.02670288085938\n",
      "Memory after fetching train data : \n",
      "Total memory = 127.99956130981445; used memory = 44.997650146484375; available memory = 83.00191116333008\n",
      "power bi count 198558\n",
      "Memory after generating power bi sample : \n",
      "Total memory = 127.99956130981445; used memory = 45.03152084350586; available memory = 82.9680404663086\n",
      "closing memory : \n",
      "Total memory = 127.99956130981445; used memory = 45.06442642211914; available memory = 82.93513488769531\n"
     ]
    }
   ],
   "source": [
    "print(str(datetime.datetime.today()))\n",
    "tenant='aduxp'\n",
    "aduxproles = ['ADDataService']\n",
    "for i in aduxproles:\n",
    "    print(\"Producing Efficiency View for : \" , i)\n",
    "    role = i\n",
    "    metrics=[]\n",
    "    sample,Powerbi_sample,prediction,parameters,path = efficiency(tenant,role,asofdate,storageaccountname,accountkey,container,No_PBI_samples,Sample_Percent,random_State,cv,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit)\n",
    "    push_bothpredictions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbiprediction_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbisample_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,Powerbi_sample)\n",
    "    push_trainingdata_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample)\n",
    "    push_metrics_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,parameters)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-20 00:38:52.314059\n",
      "Producing Efficiency View for :  ADExtension\n",
      "Opening memory : \n",
      "Total memory = 127.99956130981445; used memory = 44.71616744995117; available memory = 83.28339385986328\n",
      "Memory after loading data : \n",
      "Total memory = 127.99956130981445; used memory = 44.81513977050781; available memory = 83.18442153930664\n",
      "Memory after generating dates : \n",
      "Total memory = 127.99956130981445; used memory = 44.80695343017578; available memory = 83.19260787963867\n",
      "% of Data between standard deviation range -1 & 1 : 0.6470727486809946\n",
      "% of Data between standard deviation range -2 & 2 : 0.9538142730829262\n",
      "% of Data between standard deviation range -3 & 3 : 0.9981052960728838\n",
      "Number of rows after cleaning data: 1129958\n",
      "Memory after filtering by standard deviation : \n",
      "Total memory = 127.99956130981445; used memory = 44.7913818359375; available memory = 83.20817947387695\n",
      "Number of rows after filtering by influence: 1129957\n",
      "sample count 790969\n",
      "Memory after filtering by influence and sampling : \n",
      "Total memory = 127.99956130981445; used memory = 44.794334411621094; available memory = 83.20522689819336\n",
      "RET_lower: 5 RET_upper: 2020 RET_step: 202\n",
      "RAM_lower: 2 RAM_upper: 5 RAM_step: 1\n",
      "Memory after building prediction data : \n",
      "Total memory = 127.99956130981445; used memory = 44.794334411621094; available memory = 83.20522689819336\n",
      "Fitting MARS model\n",
      "Earth Model\n",
      "---------------------------------------------------------\n",
      "Basis Function                      Pruned  Coefficient  \n",
      "---------------------------------------------------------\n",
      "(Intercept)                         No      42.9371      \n",
      "h(PercentProcessorTime-1.713)       Yes     None         \n",
      "h(1.713-PercentProcessorTime)       No      -1.24532     \n",
      "h(PercentProcessorTime-6.6981)      Yes     None         \n",
      "h(6.6981-PercentProcessorTime)      No      -5.13461     \n",
      "h(PercentProcessorTime-4.8342)      No      -0.331942    \n",
      "h(4.8342-PercentProcessorTime)      Yes     None         \n",
      "h(Percent_Memory_Utilized-10.1898)  No      0.223654     \n",
      "h(10.1898-Percent_Memory_Utilized)  No      -0.0783508   \n",
      "h(PercentProcessorTime-6.5515)      Yes     None         \n",
      "h(6.5515-PercentProcessorTime)      No      6.47955      \n",
      "h(PercentProcessorTime-12.7963)     No      14.2084      \n",
      "h(12.7963-PercentProcessorTime)     Yes     None         \n",
      "h(PercentProcessorTime-16.3729)     Yes     None         \n",
      "h(16.3729-PercentProcessorTime)     No      -1.54966     \n",
      "h(PercentProcessorTime-11.9351)     No      -11.0863     \n",
      "h(11.9351-PercentProcessorTime)     Yes     None         \n",
      "h(PercentProcessorTime-5.8314)      Yes     None         \n",
      "h(5.8314-PercentProcessorTime)      No      -10.0456     \n",
      "h(PercentProcessorTime-14.5446)     Yes     None         \n",
      "h(14.5446-PercentProcessorTime)     No      -3.12432     \n",
      "h(PercentProcessorTime-6.3527)      No      -2.9625      \n",
      "h(6.3527-PercentProcessorTime)      No      12.7351      \n",
      "---------------------------------------------------------\n",
      "MSE: 0.3890, GCV: 0.3890, RSQ: 0.5778, GRSQ: 0.5778\n",
      "Fitting LR model\n",
      "cross val mse: 0.660383\n",
      "R2: 0.28\n",
      "mean absoluter error: 0.429757\n",
      "Mean squared error: 0.67\n",
      "Memory after building the model : \n",
      "Total memory = 127.99956130981445; used memory = 44.7257194519043; available memory = 83.27384185791016\n",
      "Memory after saving parameters : \n",
      "Total memory = 127.99956130981445; used memory = 44.7257194519043; available memory = 83.27384185791016\n",
      "Memory after fetching train data : \n",
      "Total memory = 127.99956130981445; used memory = 44.741363525390625; available memory = 83.25819778442383\n",
      "power bi count 48448\n",
      "Memory after generating power bi sample : \n",
      "Total memory = 127.99956130981445; used memory = 44.756141662597656; available memory = 83.2434196472168\n",
      "closing memory : \n",
      "Total memory = 127.99956130981445; used memory = 44.778228759765625; available memory = 83.22133255004883\n"
     ]
    }
   ],
   "source": [
    "print(str(datetime.datetime.today()))\n",
    "tenant='msods'\n",
    "ibizaroles = ['adminwebservice']\n",
    "for i in ibizaroles:\n",
    "    print(\"Producing Efficiency View for : \" , i)\n",
    "    role = i\n",
    "    metrics=[]\n",
    "    sample,Powerbi_sample,prediction,parameters,path = efficiency(tenant,role,asofdate,storageaccountname,accountkey,container,No_PBI_samples,Sample_Percent,random_State,cv,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit)\n",
    "    push_bothpredictions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbiprediction_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbisample_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,Powerbi_sample)\n",
    "    push_trainingdata_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample)\n",
    "    push_metrics_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,parameters)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-20 20:56:56.284492\n",
      "Producing Efficiency View for :  AzurePortalWebsite\n",
      "Opening memory : \n",
      "Total memory = 127.99956130981445; used memory = 46.25084686279297; available memory = 81.74871444702148\n",
      "Memory after loading data : \n",
      "Total memory = 127.99956130981445; used memory = 46.51201629638672; available memory = 81.48754501342773\n",
      "Memory after generating dates : \n",
      "Total memory = 127.99956130981445; used memory = 46.459190368652344; available memory = 81.54037094116211\n",
      "% of Data between standard deviation range -1 & 1 : 0.664258342212415\n",
      "% of Data between standard deviation range -2 & 2 : 0.9607406072389184\n",
      "% of Data between standard deviation range -3 & 3 : 0.9977177413857344\n",
      "Number of rows after cleaning data: 3205275\n",
      "Memory after filtering by standard deviation : \n",
      "Total memory = 127.99956130981445; used memory = 46.42683029174805; available memory = 81.5727310180664\n",
      "Number of rows after filtering by influence: 3205274\n",
      "sample count 2243691\n",
      "Memory after filtering by influence and sampling : \n",
      "Total memory = 127.99956130981445; used memory = 46.430137634277344; available memory = 81.56942367553711\n",
      "RET_lower: 0 RET_upper: 3 RET_step: 1\n",
      "RAM_lower: 11 RAM_upper: 15 RAM_step: 1\n",
      "Memory after building prediction data : \n",
      "Total memory = 127.99956130981445; used memory = 46.430137634277344; available memory = 81.56942367553711\n",
      "Fitting MARS model\n",
      "Earth Model\n",
      "----------------------------------------------------------\n",
      "Basis Function                      Pruned  Coefficient   \n",
      "----------------------------------------------------------\n",
      "(Intercept)                         No      -1.19218e+07  \n",
      "h(PercentProcessorTime-10.8603)     Yes     None          \n",
      "h(10.8603-PercentProcessorTime)     No      -154.535      \n",
      "h(PercentProcessorTime-30.4354)     Yes     None          \n",
      "h(30.4354-PercentProcessorTime)     No      -44.2444      \n",
      "h(RequestExecutionTime-10.9944)     No      30.2333       \n",
      "h(10.9944-RequestExecutionTime)     No      95.1502       \n",
      "h(Percent_Memory_Utilized-19.1773)  No      20362.8       \n",
      "h(19.1773-Percent_Memory_Utilized)  Yes     None          \n",
      "h(Percent_Memory_Utilized-20.246)   No      -22376.5      \n",
      "h(20.246-Percent_Memory_Utilized)   No      -26770.4      \n",
      "h(Percent_Memory_Utilized-8.12071)  Yes     None          \n",
      "h(8.12071-Percent_Memory_Utilized)  No      27129.7       \n",
      "h(Percent_Memory_Utilized-7.96573)  No      -26774.8      \n",
      "h(7.96573-Percent_Memory_Utilized)  Yes     None          \n",
      "h(PercentProcessorTime-25.136)      No      963782        \n",
      "h(25.136-PercentProcessorTime)      Yes     None          \n",
      "h(PercentProcessorTime-25.1326)     Yes     None          \n",
      "h(25.1326-PercentProcessorTime)     No      -964431       \n",
      "h(PercentProcessorTime-21.9136)     No      700.984       \n",
      "h(21.9136-PercentProcessorTime)     Yes     None          \n",
      "h(PercentProcessorTime-37.8326)     No      -964618       \n",
      "h(37.8326-PercentProcessorTime)     No      969090        \n",
      "h(PercentProcessorTime-46.5871)     Yes     None          \n",
      "h(46.5871-PercentProcessorTime)     No      -96.5641      \n",
      "h(PercentProcessorTime-37.6385)     Yes     None          \n",
      "h(37.6385-PercentProcessorTime)     No      -4444.18      \n",
      "h(PercentProcessorTime-15.1781)     No      121.963       \n",
      "h(15.1781-PercentProcessorTime)     Yes     None          \n",
      "h(PercentProcessorTime-76.2081)     No      91.4801       \n",
      "h(76.2081-PercentProcessorTime)     Yes     None          \n",
      "----------------------------------------------------------\n",
      "MSE: 303087.8771, GCV: 303100.3053, RSQ: 0.0840, GRSQ: 0.0839\n",
      "Fitting LR model\n",
      "cross val mse: 324861.672716\n",
      "R2: 0.02\n",
      "mean absoluter error: 204.494277\n",
      "Mean squared error: 343138.28\n",
      "Memory after building the model : \n",
      "Total memory = 127.99956130981445; used memory = 46.52437973022461; available memory = 81.47518157958984\n",
      "Memory after saving parameters : \n",
      "Total memory = 127.99956130981445; used memory = 46.524383544921875; available memory = 81.47517776489258\n",
      "Memory after fetching train data : \n",
      "Total memory = 127.99956130981445; used memory = 46.5319938659668; available memory = 81.46756744384766\n",
      "power bi count 400754\n",
      "Memory after generating power bi sample : \n",
      "Total memory = 127.99956130981445; used memory = 46.59638595581055; available memory = 81.4031753540039\n",
      "closing memory : \n",
      "Total memory = 127.99956130981445; used memory = 46.655799865722656; available memory = 81.3437614440918\n"
     ]
    }
   ],
   "source": [
    "print(str(datetime.datetime.today()))\n",
    "tenant='iamux'\n",
    "roles = ['AzurePortalWebsite']\n",
    "for i in roles:\n",
    "    print(\"Producing Efficiency View for : \" , i)\n",
    "    role = i\n",
    "    metrics=[]\n",
    "    sample,Powerbi_sample,prediction,parameters,path = efficiency(tenant,role,asofdate,storageaccountname,accountkey,container,No_PBI_samples,Sample_Percent,random_State,cv,cpu_bucket,memory_bucket,cpuvsrps_rpsbucket,cpuvsrps_rps_limit)\n",
    "    push_bothpredictions_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbiprediction_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,prediction)\n",
    "    push_pbisample_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,Powerbi_sample)\n",
    "    push_trainingdata_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,path,sample)\n",
    "    push_metrics_toblob(storageaccountname,accountkey,tenant,role,container,asofdate,parameters)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
